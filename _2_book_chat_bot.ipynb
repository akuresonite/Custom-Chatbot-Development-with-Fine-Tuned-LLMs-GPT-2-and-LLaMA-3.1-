{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Book Chat Bot}$\n",
    "\n",
    "Fine Tuned Large Language Model like GPT-2 and Llama-3.1 on any book\n",
    "\n",
    "Author: Ashish Kumar Uchadiya\n",
    "\n",
    "Contact: akuresonite@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install transformers\n",
    "# !pip install accelerate -U\n",
    "# !pip install transformers[torch]\n",
    "# !pip install torch\n",
    "# !pip install -U PyPDF2\n",
    "# !pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: \u001b[33mcuda:1\u001b[0m\n",
      "Cpu cores found: \u001b[33m64\u001b[0m\n",
      "CUDA device: \u001b[33mNVIDIA RTX A5000\u001b[0m\n",
      "CUDA cores found: \u001b[33m4096\u001b[0m\n",
      "Last updated: 2024-08-16T22:20:57.456397+05:30\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.9\n",
      "IPython version      : 8.25.0\n",
      "\n",
      "Compiler    : GCC 11.2.0\n",
      "OS          : Linux\n",
      "Release     : 6.1.0-12-amd64\n",
      "Machine     : x86_64\n",
      "Processor   : \n",
      "CPU cores   : 64\n",
      "Architecture: 64bit\n",
      "\n",
      "torch       : 2.3.1\n",
      "transformers: 4.43.2\n",
      "\n",
      "2.3.1\n",
      "Random seed set as: 42\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from colorama import Fore, Style\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def clr(text):\n",
    "    return f\"{Fore.YELLOW}{text}{Style.RESET_ALL}\"\n",
    "def get_cuda_cores():\n",
    "    device = torch.cuda.current_device()\n",
    "    compute_capability = torch.cuda.get_device_capability(device)\n",
    "    cores_per_sm = {2: 32, 3: 192, 5: 128, 6: 64, 7: 64, 8: 64}  # cores per streaming multiprocessor\n",
    "    sm_count = torch.cuda.get_device_properties(device).multi_processor_count\n",
    "    cores = sm_count * cores_per_sm[compute_capability[0]]\n",
    "    return cores\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    import random, numpy\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as: {seed}\")\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"device:\", clr(device))\n",
    "print(\"Cpu cores found:\", clr(os.cpu_count()))\n",
    "try:\n",
    "    print(\"CUDA device:\", clr(torch.cuda.get_device_name(device=device)))\n",
    "    print(f\"CUDA cores found: {clr(get_cuda_cores())}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from watermark import watermark\n",
    "print(watermark())\n",
    "print(watermark(packages=\"torch,transformers\"))\n",
    "print(torch.__version__)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def read_word(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def read_documents_from_directory(directory):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            combined_text += read_pdf(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            combined_text += read_word(file_path)\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# GPT2Tokenizer.from_pretrained('openai-community/gpt2')\n",
    "# GPT2LMHeadModel.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "# GPT2Tokenizer.from_pretrained('openai-community/gpt2-medium')\n",
    "# GPT2LMHeadModel.from_pretrained('openai-community/gpt2-medium')\n",
    "\n",
    "# GPT2Tokenizer.from_pretrained('openai-community/gpt2-large')\n",
    "# GPT2LMHeadModel.from_pretrained('openai-community/gpt2-large')\n",
    "\n",
    "# GPT2Tokenizer.from_pretrained('openai-community/gpt2-xl')\n",
    "# GPT2LMHeadModel.from_pretrained('openai-community/gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20K\tlogs\n",
      "24K\t_2_book_chat_bot.ipynb\n",
      "28K\thelper_functions.py\n",
      "32K\t_1_GPT2_FT.ipynb\n",
      "1.5M\tdata\n",
      "3.3G\tfinetuned\n",
      "9.6G\t/home/23m1521/.cache/huggingface/hub/\n"
     ]
    }
   ],
   "source": [
    "!du \"/home/23m1521/.cache/huggingface/hub/\" -sh $(ls -A) | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_chatbot(directory, model_name, train_fraction=0.8):\n",
    "    \n",
    "    model_output_path = os.path.join('finetuned', model_name)\n",
    "    os.makedirs(model_output_path, exist_ok=True)\n",
    "   \n",
    "    # combined_text = read_documents_from_directory(directory)\n",
    "    combined_text = read_txt(directory)\n",
    "    combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n",
    "\n",
    " \n",
    "    split_index = int(train_fraction * len(combined_text))\n",
    "    train_text = combined_text[:split_index]\n",
    "    val_text = combined_text[split_index:]\n",
    "    \n",
    "    print(f\"combined_text:{len(combined_text)}, train:{len(train_text)}, val:{len(val_text)}\")\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    with open(\"data/train.txt\", \"w\") as f:\n",
    "        f.write(train_text)\n",
    "    with open(\"data/val.txt\", \"w\") as f:\n",
    "        f.write(val_text)\n",
    "\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model directly\n",
    "    # from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"data/train.txt\", block_size=128)\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"data/val.txt\", block_size=128)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    " \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_path,\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(model_output_path)\n",
    "    tokenizer.save_pretrained(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=100):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids) # Create the attention mask and pad token id\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def print_response(response):\n",
    "    for i, word in enumerate(response.split(\" \")):\n",
    "        if (i > 0) and (i%10 == 0):\n",
    "            print()\n",
    "        print(word.strip(), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Book From https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_txt_file(url, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"File '{save_path}' already exists. Skipping download.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(save_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        print(f\"File downloaded and saved as '{save_path}'\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/home/23m1521/datasets/text_data/dracula.txt' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/home/23m1521/datasets/text_data'\n",
    "\n",
    "url = \"https://www.gutenberg.org/ebooks/345.txt.utf-8\"\n",
    "save_path = os.path.join(dataset_dir, \"dracula.txt\")\n",
    "\n",
    "download_txt_file(url, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models = ['openai-community/gpt2','openai-community/gpt2-medium', 'openai-community/gpt2-large','openai-community/gpt2-xl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data logs finetuned -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'openai-community/gpt2'\n",
    "model_output_path = os.path.join('finetuned', MODEL_NAME)\n",
    "# filepath = '/home/23m1521/datasets/text_data/Movies_plot_txt/The_Avengers.txt'\n",
    "filepath = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_text:862701, train:690160, val:172541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/23m1521/.conda/envs/cuda_env/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/home/23m1521/.conda/envs/cuda_env/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18300' max='18300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18300/18300 27:11, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.772100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.852200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.607900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.438900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.085800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/23m1521/.conda/envs/cuda_env/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20K\tlogs\n",
      "24K\t_2_book_chat_bot.ipynb\n",
      "28K\thelper_functions.py\n",
      "32K\t_1_GPT2_FT.ipynb\n",
      "1.5M\tdata\n",
      "3.3G\tfinetuned\n",
      "CPU times: user 31min 24s, sys: 46.9 s, total: 32min 11s\n",
      "Wall time: 27min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_chatbot(filepath, MODEL_NAME)\n",
    "\n",
    "!du -sh $(ls -A) | sort -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Fine-Tuned model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_output_path).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was Dracula? Count Dracula is an ancient vampire and Transylvanian nobleman who \n",
      "feeds on the blood of the living to maintain his \n",
      "immortality. He possesses supernatural abilities such as shapeshifting, mind control, \n",
      "and enhanced strength. Dracula resides in a decaying castle in \n",
      "the Carpathian Mountains and plans to expand his vampiric influence \n",
      "by relocating to England. The novel's protagonist, Jonathan Harker, first \n",
      "encounters Dracula when he travels to Transylvania to assist him \n",
      "with a real estate transaction. As Dracula begins to prey \n",
      "on victims in England, a group led by Professor Van \n",
      "Helsing bands together to hunt him down. Dracula represents the \n",
      "terror of the unknown and the clash between modern civilization \n",
      "and ancient evil. \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Who was Dracula'\n",
    "    ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    print_response(response)\n",
    "    print(\"-\"*37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "model_output_path = os.path.join('finetuned', MODEL_NAME)\n",
    "filepath = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_chatbot(filepath, MODEL_NAME)\n",
    "\n",
    "!du -sh $(ls -A) | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Who was Dracula'\n",
    "    ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    print_response(response)\n",
    "    print(\"-\"*37)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
